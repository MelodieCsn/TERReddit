{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Classification de données textuelles </H1>\n",
    "\n",
    "\n",
    "Lors de l'étape d'ingénierie de données textuelles nous avons vu que diverses opérations pouvaient être appliquées sur les textes et qu'au final il est possible d'obtenir des textes simplifiés. Nous allons, à présent, étudier comment faire de la classification à partir de données textuelles et comment convertir les textes en vecteurs pour pouvoir faire de la classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'un document a été transformé en une séquence de mot, il est nécessaire de la transformer en vecteur. C'est le rôle de la vectorisation.  \n",
    "\n",
    "\n",
    "\n",
    "** Bag of Words **\n",
    "\n",
    "La manière la plus simple de vectorisation est d'utiliser les Bag of Words (BOW). Il s'agit, à partir d'une liste de mots (vocabulaire) de compter le nombre d'apparition du mot du vocabulaire dans le document.  \n",
    "\n",
    "Cette opération se fait par :\n",
    "1. Création d'une instance de la classe CountVectorizer\n",
    "1. Appel de la fonction fit() pour apprendre le vocabulaire à partir de document\n",
    "1. Appel de la fonction transform() sur un ou plusieurs documents afin de les encoder dans le vecteur.  \n",
    "\n",
    "Attention, par défaut, CountVectorizer effectue un certain nombre de pré-traitements comme par exemple mise en minuscule. Voir https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texte = [\"This is an example of CountVectorizer for creating a vector\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# creation du vocabulaire\n",
    "vectorizer.fit(texte)\n",
    "# Contenu du vocabulaire\n",
    "print(vectorizer.vocabulary_)\n",
    "# encodage du document\n",
    "vector = vectorizer.transform(texte)\n",
    "\n",
    "print (\"Taille du vecteur :\\n\",vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est donc à présent possible de traiter un ensemble de documents comme le montre l'exemple suivant. Nous créons également un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is my first document.',\n",
    "    'This is the document 2 !',\n",
    "    'Maybe this is the third document?',\n",
    "    'Anything else? may be 40',\n",
    "    'Yes !! this is the last one'\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# vectorizer.get_feature_names())\n",
    "# contient le vocabulaire\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prise en compte des prétraitements  **  \n",
    "\n",
    "Considérons les prétraitements suivants qui permettent de supprimer les caractères non Ascii, de mettre en minuscule, d'enlever les ponctuations, de remplacer les nombres et d'enlever les stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word) \n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens=normalize(tokens)\n",
    "    text=\"\".join([\" \"+i for i in tokens]).strip()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte=\"we have to think that is text is 1000 *#better than. the \"\n",
    "print (\"Texte avant le nettoyage \\n\")\n",
    "print (texte)\n",
    "texte=clean_text(texte)\n",
    "print (\"Texte après le nettoyage \\n\")\n",
    "print (texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'appel aux fonctions de prétraitements peut se faire directement dans CountVectorizer. Attention cependant il est préférable de ne pas le faire. Par exemple dans le cas d'un pipeline et d'un gridsearch le prétraitement sera effectué à chaque fois ! Il est par contre utile de le faire lors de la dernière étape et que le modèle est sauvegardé pour permettre qu'un nouveau document puisse être transformé avant d'être mis sous la forme d'un vecteur (voir plus bas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(corpus))\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i]=clean_text(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ici le preprocessor ne sert à rien\n",
    "# car les données ont été nettoyées avant.\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=clean_text\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appel dans CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "    'This is my first document.',\n",
    "    'This is the document 2 !',\n",
    "    'Maybe this is the third document?',\n",
    "    'Anything else? may be 40',\n",
    "    'Yes !! this is the last one'\n",
    "]\n",
    "#Rappel ce n'est pas efficace\n",
    "#il vaut mieux traiter les données avant\n",
    "#attention aux pipelines\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=clean_text\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TfidfVectorizer **\n",
    "\n",
    "CountVectorizer en prenant en compte l'occurrence des mots est souvent trop limité. Une alternative est d'utiliser la \n",
    "mesure TF-IDF (Term Frequency – Inverse Document) via une instance de la classe TfidfVectorizer. Le principe est le même que pour CountVectorizer.\n",
    "\n",
    "Remarque : Il est possible si CountVectorizer a déjà été utilisé de le faire suivre par TfidfTransformer pour simplement mettre à jour les valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is my first document.',\n",
    "    'This is the document 2 !',\n",
    "    'Maybe this is the third document?',\n",
    "    'Anything else? may be 40',\n",
    "    'Yes !! this is the last one'\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# vectorizer.get_feature_names())\n",
    "# contient le vocabulaire\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel d'un pré-prétraiment\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=clean_text\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appel de fonction de prétraitement dans TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel d'un pré-prétraiment\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=clean_text\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Les ngrams **\n",
    "\n",
    "Très souvent il est utile de prendre en compte les n-grammes, i.e. la suite de n mots consécutifs car ils peuvent être important pour la classification. Il est tout à fait possible de les obtenir soit pendant l'étape de prétraitement, soit lors de l'étape de vectorisation en le passant en paramètre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel d'un pré-prétraiment\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=clean_text,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vectorizer.transform(corpus).toarray(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir du moment où nous disposons d'une matrice, nous pouvons appliquer toutes les approches que nous avons vu précédement.  \n",
    "Nous illustrons au travers d'un exemple de classification multiclasse. Ce dernier est tiré de \"The 20 newsgroups text dataset\". Il s'agit d'un jeu de données de 20 newsgroups comprenant à peu près 18000 news sur 20 sujets différents. \n",
    "\n",
    "Ce jeu de données est disponible sous scikit learn qui propose des fonctions pour le manipuler : https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset  \n",
    "\n",
    "fetch_20newsgroups permet de charger le fichier. Il est possible de récupérer un jeu d'entrainement, de test ou l'ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Présentation du jeu de données **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-276c075bda06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"liste des topics \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "from pprint import pprint\n",
    "print (\"liste des topics \\n\")\n",
    "pprint(list(news.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Téléchargement d'une partie des topics **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'rec.sport.hockey','comp.graphics', 'sci.space']\n",
    "\n",
    "news = fetch_20newsgroups(subset='all',\n",
    "                          categories=categories)\n",
    "\n",
    "print (\"Taille du jeu de données\\n\")\n",
    "print (news.filenames.shape)\n",
    "\n",
    "print (\"Un exemple de données\\n\")\n",
    "print (news.data[5], '\\n TOPIC : ',news.target[5],\n",
    "       '\\n*******************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Nettoyage des données **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_news (data):\n",
    "    for i in range(len(data)):\n",
    "        #print (i)\n",
    "        data[i]=clean_text(data[i])\n",
    "    return data    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.data=clean_news(news.data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Vectorisation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(news.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** X et y **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification des variables à prédire et de la classe X et y\n",
    "X = vectors.toarray()\n",
    "\n",
    "y = news.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Création du jeu d'apprentissage et de test **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "validation_size=0.3 #30% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utilisation du classifieur **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from time import time\n",
    "\n",
    "seed=7\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "clf = GaussianNB()\n",
    "\n",
    "scoring = 'accuracy'\n",
    "t0 = time()\n",
    "score = cross_val_score(clf, X, y, cv=k_fold, scoring=scoring)\n",
    "print(\"Réalisé en %0.3fs\" % (time() - t0))\n",
    "\n",
    "print('Les différentes accuracy pour les 10 évaluations sont : \\n',\n",
    "      score,'\\n')\n",
    "print ('Accuracy moyenne : ',score.mean(), \n",
    "       ' standard deviation', score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mise en place d'un pipeline **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', \n",
    "                                      penalty='l2',\n",
    "                                      alpha=1e-3, \n",
    "                                      random_state=42, \n",
    "                                      max_iter=5, tol=None)),\n",
    "               ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X=news.data\n",
    "y=news.target\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Fit réalisé en %0.3fs\" % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "result = pipeline.predict(X_test)\n",
    "print(\"Prédiction réalisée en %0.3fs\" % (time() - t0))\n",
    "\n",
    "print('\\n accuracy:',accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "\n",
    "\n",
    "\n",
    "print ('\\n',classification_report(y_test, result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mise en place d'un gridsearch avec pipeline pour rechercher le meilleur classifieur **\n",
    "\n",
    "Dans cette section nous intégrons un pipeline complet : lancement du TfidfVectorizer , utilisation de deux classifieurs (DecisionTreeClassifier et SGDClassifier) avec les hyperparamètres  pour évaluer le meilleur via GridSearchCV. Attention le processus de gridsearch est très long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Specification des pipelines\n",
    "# programmation à optimiser par une fonction :)\n",
    "pipeline_SGDC = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                    ('clf', SGDClassifier())])\n",
    "\n",
    "\n",
    "parameters_SGDC = [\n",
    "    {'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet')}\n",
    "]\n",
    "\n",
    "pipeline_DT = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                   ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "\n",
    "#param_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "param_range = [1, 5, 8, 10]\n",
    "parameters_DT = [\n",
    "    {'clf__min_samples_leaf': param_range,\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': param_range,\n",
    "        'clf__min_samples_split': param_range[1:]}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X=news.data\n",
    "y=news.target\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "# Creation des GridSearchCV avec les pipelines spécifiques\n",
    "\n",
    "gs_SGDC = GridSearchCV(pipeline_SGDC, \n",
    "                       parameters_SGDC, \n",
    "                       cv=3,\n",
    "                       n_jobs=-1, \n",
    "                       scoring='accuracy')\n",
    "\n",
    "\n",
    "gs_DT = GridSearchCV(pipeline_DT, \n",
    "                     parameters_DT, \n",
    "                     cv=3,\n",
    "                     n_jobs=-1, \n",
    "                     scoring='accuracy')\n",
    "\n",
    "\n",
    "\n",
    "grids = [gs_SGDC, gs_DT]\n",
    "grid_dict={0:'Linear classifiers', 1:'Decision Tree'}\n",
    "\n",
    "best_acc = 0.0\n",
    "best_clf = 0.0\n",
    "best_gs = ''\n",
    "\n",
    "for idx,gs in enumerate(grids):\n",
    "    print('\\nClassifier: %s' % grid_dict[idx])\n",
    "    t0 = time()\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Fit réalisé en %0.3fs\" % (time() - t0))\n",
    "\n",
    "    print('Meilleurs paramètres : %s' % gs.best_params_)\n",
    "\n",
    "    print(\"Meilleur score d'accuracy sur l'entrainement: %.3f\" % gs.best_score_)\n",
    "    # Prediction sur le jeu de test avec les meilleurs paramètres\n",
    "    t0 = time()\n",
    "    result = gs.predict(X_test)\n",
    "    print(\"Prédiction réalisée en %0.3fs\" % (time() - t0))\n",
    "    \n",
    "    print(\"Score d'accuracy pour les meilleurs paramètres sur jeu de test : %.3f\"  % accuracy_score(y_test, result))\n",
    "\n",
    "    print ('\\n matrice de confusion \\n',confusion_matrix(y_test, result))\n",
    "\n",
    "    print ('\\n',classification_report(y_test, result))\n",
    "    \n",
    "    #Modele avec la meilleure accuracy sur le jeu de test\n",
    "    if accuracy_score(y_test, result) > best_acc:\n",
    "        best_acc = accuracy_score(y_test, result)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "        \n",
    "        \n",
    "        \n",
    "print('\\nClassifier avec la meilleur accuracy sur le jeu de test\\n',\n",
    "      grid_dict[best_clf])        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recupération du meilleur classifieur avec ses paramètres **\n",
    "\n",
    "Une fois le résultat obtenu, il est possible de récupérer le meilleur classifieur et ses paramètres. Ici il s'agit de Linear Classifier (SGDClassifier). Dans la suite nous montrons comment relancer une classification via un pipeline et une sauvegarde. Attention ici dans le pipeline nous ajoutons le clean_text (cf remarques précédentes) et nous sauvegardons le modèle pour pouvoir l'utiliser après avec d'autres données.   \n",
    "\n",
    "Le fait de mettre le clean_text dans le pipeline permet lors de la sauvegarde de celui-ci de pouvoir lorsqu'il y a de nouvelles données de les relancer dans le pipeline (les donnés récupéreront dont la matrice du TfidfVectorizer et les pré-traitements associés). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#Recupération des données pour l'exemple\n",
    "#et partir proprement\n",
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'rec.sport.hockey','comp.graphics', 'sci.space']\n",
    "\n",
    "news = fetch_20newsgroups(subset='all',\n",
    "                          categories=categories)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(preprocessor=clean_text)),\n",
    "                ('clf', SGDClassifier(loss='hinge', \n",
    "                                      penalty='l2',\n",
    "                                      alpha=1e-05, \n",
    "                                      random_state=42, \n",
    "                                      max_iter=5, \n",
    "                                      tol=None)),\n",
    "               ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X=news.data\n",
    "y=news.target\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "print (\"Lancement du fit \\n\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Fit réalisé en %0.3fs\" % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "print (\"Lancement de la prédiction \\n\")\n",
    "result = pipeline.predict(X_test)\n",
    "print(\"Prédiction réalisée en %0.3fs\" % (time() - t0))\n",
    "\n",
    "print('\\n accuracy:',accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "\n",
    "\n",
    "\n",
    "print ('\\n',classification_report(y_test, result))\n",
    "\n",
    "print(\"\\nSauvegarde du pipeline grid search\") \n",
    "filename = 'thebestone1.pkl'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utilisation de nouvelles données **  \n",
    "\n",
    "L'objectif ici est d'utiliser de nouvelles données à partir du modèle appris. Lors de la sauvegarde le pipeline entier a été sauvegardé. Cela implique que lorsque l'on va vouloir prédire les prétraitements du pipeline vont être appliqués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (\"Chargement du modèle \\n\")\n",
    "filename = 'thebestone.pkl'\n",
    "clf_loaded = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "print (\"A partir d'un nouveau texte\\n\")\n",
    "print (\"Utilisation d'un texte de 20newsgroup\\n\")\n",
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'rec.sport.hockey','comp.graphics', 'sci.space']\n",
    "\n",
    "news = fetch_20newsgroups(subset='all',\n",
    "                          categories=categories)\n",
    "\n",
    "print (\"Sélection aléatoire de 20 documents \\n\")\n",
    "from random import randint\n",
    "samples=[]\n",
    "samples_result=[]\n",
    "sample_new=[]\n",
    "for i in range(1,20):\n",
    "    val=randint(1,4385)\n",
    "    sample_new.append(val)\n",
    "    samples.append(news.data[val])\n",
    "    samples_result.append(news.target[val])\n",
    "    \n",
    "print (\"Prédiction des news séléctionnées\\n\")    \n",
    "    \n",
    " \n",
    "\n",
    "result = clf_loaded.predict(samples)\n",
    "\n",
    "print (\"Valeurs réelles vs. valeurs prédites\\n\") \n",
    "for i in range(len(result)):\n",
    "    print (\"News : \",sample_new[i], \n",
    "           \"\\t réelle \", \n",
    "           samples_result[i], \n",
    "           \" prédite \",\n",
    "           result [i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
